---
title: Multilevel Modeling and Climate Data Analysis Workshop
subtitle: Using R to add context to population health research
author:
    - name: |
        <p style='font-size: 25px; margin-left: 50px'>
          <img src='images/finn.png' width='100' style='vertical-align: middle'>
          Finn Roberts <span style='opacity: 0.5;'>- IPUMS Senior Data Analyst</span>
        </p>
    - name: |
        <p style='font-size: 25px; margin-left: 50px'>
          <img src='images/kat.png' width='100' style='vertical-align: middle'>
          Kathryn Grace <span style='opacity: 0.5;'>- UMN Department of Geography</span>
        </p>
format:
  revealjs:
    theme: [default, custom.scss]
    logo: images/dhs_logo_navy.png
    chalkboard: true
    smaller: true
    scrollable: false
    incremental: true
    # code-overflow: wrap
    # preview-links: true
editor_options: 
  chunk_output_type: inline
execute:
  echo: true
knitr:
  opts_chunk: 
    collapse: true
    comment: "#>"
---

```{r}
#| results: hide
#| echo: false
# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # truncate the output
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})

source("R/utils.R")

# Load fonts
# sysfonts::font_add(
#   family = "cabrito",
#   regular = "../../fonts/cabritosansnormregular-webfont.ttf"
# )
#
# showtext::showtext_auto()
```

# Background

## DHS + IPUMS DHS

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   [The Demographic and Health Surveys
    Program](https://dhsprogram.com/) (DHS) is a leading source of
    population health data in many low- and middle- income countries
:::

::: {.fragment .fade-in fragment-index="2"}
-   IPUMS DHS standardizes DHS survey data across time and space
:::

::: {.fragment .fade-in fragment-index="2"}
![](images/ipums_dhs.png){fig-align="center" height="350"}
:::
:::

## Our Core Question

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   What about people's environmental context?
:::

::: {.fragment .fade-in fragment-index="2"}
-   How do we understand how climate and environmental conditions are
    related to population health?
:::

::: {.fragment .fade-in fragment-index="2"}
![](images/drought.jpg){.absolute left="20" bottom="120" height="250"}
![](images/storm.jpg){.absolute right="20" bottom="50" height="350"}
:::
:::

## Using IPUMS DHS with Climate Data

-   Pre-made environmental contextual variables?

    -   Not very flexible...

-   You can't define environmental exposure in the way most appropriate
    for *your* research

## Using IPUMS DHS with Climate Data

-   DHS distributes **GPS coordinates** for the areas that are surveyed

-   We can obtain data from *external* climate sources

-   Then **attach** to the DHS survey results *using these coordinates*

## A Motivating Example

-   Does extreme heat impact child birth weight outcomes?

-   **DHS survey data:** Kenya 2014 Sample

-   **Climate data:** CHIRTS

# Outline

1.  Setup
2.  Introduction to DHS survey data
3.  Work with DHS GPS coordinates
4.  Visualize spatial data
5.  Work with CHIRTS temperature data
6.  Aggregate spatial data
7.  Link climate data and DHS surveys
8.  Build a multilevel model to assess climate impacts

# 1. Setup

## Setup

::: nonincremental
-   Comprehensive instructions can be found in the [workshop
    README](https://github.com/IPUMS-Global-Health/umn-nairobi-2024/tree/main?tab=readme-ov-file#july-2024-umn-data-analysis-workshop)

::: {.fragment .fade-in}
-   Setup includes:

    -   Registering for DHS / IPUMS DHS

    -   Creating an IPUMS DHS data extract

    -   Installing R + RStudio

    -   Downloading workshop files and launching the workshop R project

    -   Installing necessary R packages
:::
:::

# Let's get set up

[Link to workshop
materials](https://github.com/IPUMS-Global-Health/umn-nairobi-2024/tree/main?tab=readme-ov-file#july-2024-umn-data-analysis-workshop)

# 2. R Basics

## What is R?

-   Statistical programming language
-   Open-source and completely free
-   Commonly used in social sciences, statistics, environmental
    sciences, and more

![](images/r_logo.png){.absolute top="-20" right="50" height="200"}

## Why R?

::: {columns}
::: {.column width="60%"}
-   You might already use other statistical software (SPSS, SAS, Excel,
    etc.)

-   Why use R?

    -   Reproducibility + automation
    -   **Advanced statistics**
    -   Visualization
    -   **Spatial data**
    -   Advanced documentation (websites, presentations, and more)
:::
:::

![](images/r_logo.png){.absolute top="-20" right="50" height="200"}

## RStudio

-   Integrated Development Environment (IDE) dedicated to R
-   A sandbox for you to easily write, edit, and run R code

![](images/rstudio_logo.png){.absolute top="-20" right="30" height="80"}

::: {.fragment .fade-in}
![](images/rstudio.png){fig-align="center" height="400"}
:::

# R Demo

Introduction to R + RStudio üíª

::: notes
RStudio layout -- editor vs. console, environment, files, etc.
Assignment Data types Basic arithmetic Comments / whitespace Function
help documentation Packages Troubleshooting techniques Basic
functions/functional programming Function syntax Jargon (function,
argument, variable, assignment, pipe) c()! vectors, data.frames, etc.

This should also include things about how to use the materials (e.g. slides_code.Rmd)
:::

## Load R Packages

Everyone should now have the necessary packages installed.

::: {.fragment .fade-in}
We can load them by running the following code chunk in `slides_code.Rmd`

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(ipumsr)
library(sf)
library(terra)
library(ggspatial)
```
:::

# Questions?

# 3. Tabular DHS Data

## Loading DHS survey data

::: nonincremental
::: {.fragment .fade-in}
-   Recall that we downloaded **two** files for our IPUMS DHS extract

    -   The `.xml` file contains *metadata* about the file contents
    -   The `.dat.gz` file contains the actual data
:::

::: {.fragment .fade-in}
-   ipumsr makes it easy to load IPUMS data using these files

![](images/hex/ipumsr.png){fig-align="right" height="150"}
:::
:::

## Loading DHS survey data {auto-animate="true"}

```{r}
#| eval: false
# Load the metadata file
ke_ddi <- read_ipums_ddi("data/idhs_00023.xml") # Change this file name!
```

## Loading DHS survey data {auto-animate="true"}

```{r}
#| eval: false
# Load the metadata file
ke_ddi <- read_ipums_ddi("data/idhs_00023.xml") # Change this file name!

# Use metadata to load the data correctly
ke_dhs <- read_ipums_micro(ke_ddi)
```

## Loading DHS survey data {auto-animate="true"}

```{r}
#| code-line-numbers: "|8|9|10|11-20"
# Load the metadata file
ke_ddi <- read_ipums_ddi("data/idhs_00023.xml") # Change this file name!

# Use metadata to load the data correctly
ke_dhs <- read_ipums_micro(ke_ddi)

ke_dhs
```

## Exploring our DHS data

-   View table column names with `colnames()`

::: {.fragment .fade-in}
```{r}
colnames(ke_dhs)
```
:::

## Exploring our DHS data {auto-animate="true"}

-   Index individual rows and columns with `[<rows>, <cols>]`

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_dhs[98, c(1, 2, 3)]
```
:::

## Exploring our DHS data {auto-animate="true"}

::: nonincremental
-   Index individual rows and columns with `[<rows>, <cols>]`
:::

```{r}
ke_dhs[98, c(1, 2, 3)]
```

## Exploring our DHS data {auto-animate="true"}

::: nonincremental
-   Index individual rows and columns with `[<rows>, <cols>]`
:::

```{r}
#| eval: false
ke_dhs[, "DHSID"]
```

## Exploring our DHS data {auto-animate="true"}

::: nonincremental
-   Index individual rows and columns with `[<rows>, <cols>]`
:::

```{r}
ke_dhs[, "DHSID"]
```

## Exploring our DHS data

::: nonincremental
-   Use `$` to access individual columns
:::

::: {.fragment .fade-in}
```{r}
#| out.lines: 10
ke_dhs$RELIGION
```
:::

## Exploring DHS metadata {auto-animate="true"}

-   The IPUMS DHS metadata allows us to see additional details about
    each column

::: {.fragment .fade-in}
```{r}
#| eval: false
# Display variable information for all variables in our file
ipums_var_info(ke_dhs)
```
:::

## Exploring DHS metadata {auto-animate="true"}

::: nonincremental
-   The IPUMS DHS metadata allows us to see additional details about
    each column
:::

```{r}
#| out.lines: 10
# Display variable information for all variables in our file
ipums_var_info(ke_dhs)
```

## Exploring DHS metadata

-   This metadata can be useful for determining whether our variables
    need to be cleaned before analysis

::: {.fragment .fade-in}
```{r}
#| eval: false
ipums_var_desc(ke_dhs$HEIGHTFEM)
#> [1] "HEIGHTFEM (V438) reports the woman's height as measured by DHS 
#> personnel. HEIGHTFEM values are reported in millimeters, to preserve one 
#> centimeter decimal place without requiring the use of a decimal point. 
#> Dividing HEIGHTFEM by 10 will yield the woman's measured height in 
#> centimeters."
```
:::

-   In this case, we will need to adjust the decimal places and possibly
    remove missing values

## Exploring DHS metadata

-   Some variables have labeled values

    -   Categorical data

::: {.fragment .fade-in}
```{r}
#| out.lines: 10
ipums_val_labels(ke_dhs$RELIGION)
```
:::

## Exploring DHS metadata

::: nonincremental
-   Some variables have labeled values

    -   Missing values
:::

::: {.fragment .fade-in}
```{r}
ipums_val_labels(ke_dhs$HEIGHTFEM)
```
:::


## Visualizing our sample

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   Plotting data can help to identify preliminary patterns
:::

::: {.fragment .fade-in fragment-index="2"}
-   We will use `ggplot2()` to create plots that are linked to our data
:::

::: {.fragment .fade-in fragment-index="3"}
-   `ggplot2()` could be the source of an entire workshop by itself.

    -   If we have time, we can cover it during our demos
    -   But it will not be our main focus today
:::

::: {.fragment .fade-in fragment-index="2"}
![](images/hex/ggplot2.png){.absolute right="10" bottom="50"
height="150"}
:::
:::

## Visualizing our sample

-   Make a ggplot based on your data with `ggplot()`

-   Add different visual layers (`geom_`) with `+`

-   For instance, here is a histogram of our `BIRTHWT` values:

::: {.fragment .fade-in}
```{r}
#| fig-height: 4
ggplot(ke_dhs) +
  geom_histogram(aes(x = BIRTHWT))
```
:::

## Visualizing our sample

-   Obviously there is an unexpected pattern in our data!

-   We will dig into this in our exercises

# R Demo

Exploring tabular data üîç

::: notes
Mention dplyr docs...? Good starting point

Mention some key vars that folks might want to investigate and then
discuss what potential problems they noticed/things they might need to
deal with before analysis

e.g. Dates (CMC), RESIDEINT, Missing values in predictors, decimal
conversion, etc.

Discuss methods for dealing with these things

Should we do some viz here...?
:::

## Cleaning DHS data

-   As we mentioned, some variables need to be prepraed ("cleaned")
    before analysis

-   For instance, missing `BIRTHWT` values are coded as large numbers

::: {.fragment .fade-in}
```{r}
#| out.lines: 10
ke_dhs$BIRTHWT[1:5]
```
:::

## Cleaning DHS data {auto-animate="true"}

-   We can use `if_else()` to recode these values as missing (`NA`)

::: {.fragment .fade-in}
```{r}
#| eval: false
# For values greater than 9995, convert to NA (missing)
# Otherwise, divide by 1000 to convert decimal places appropriately
birthwt_clean <- if_else(
  ke_dhs$BIRTHWT > 9995, # Condition
  NA,                    # Replacement if TRUE
  ke_dhs$BIRTHWT / 1000  # Replacement if FALSE
)
```
:::

## Cleaning DHS data {auto-animate="true"}

::: nonincremental
-   We can use `if_else()` to recode these values as missing (`NA`)
:::

```{r}
# For values greater than 9995, convert to NA (missing)
# Otherwise, divide by 1000 to convert decimal places appropriately
birthwt_clean <- if_else(
  ke_dhs$BIRTHWT > 9995, # Condition
  NA,                    # Replacement if TRUE
  ke_dhs$BIRTHWT / 1000  # Replacement if FALSE
)

birthwt_clean[1:5]
```

## Cleaning DHS data {auto-animate="true"}

-   The dplyr package is designed for tabular data manipulation

-   `mutate()` allows you to modify columns

-   Use our same code above to modify the column in our data:

::: {.fragment .fade-in}
```{r}
ke_dhs <- ke_dhs |>
  mutate(BIRTHWT = if_else(BIRTHWT > 9995, NA, BIRTHWT / 1000))

ke_dhs$BIRTHWT[1:5]
```
:::

![](images/hex/dplyr.png){.absolute right="10" bottom="50" height="150"}

## Revisiting our histogram

-   Now our `BIRTHWT` distribution looks a little more reasonable

:::{.fragment .fade-in}
```{r}
#| fig-height: 4
ggplot(ke_dhs) +
  geom_histogram(aes(x = BIRTHWT), bins = 70)
```
:::

## Cleaning DHS data

-   We can do the same with several similar variables:

::: {.fragment .fade-in}
```{r}
ke_dhs <- ke_dhs |>
  mutate(
    HEIGHTFEM = if_else(HEIGHTFEM >= 9994, NA, HEIGHTFEM / 10),
    BIRTHWTREF = if_else(BIRTHWTREF >= 7, NA, BIRTHWTREF)
  )
```
:::

## Cleaning DHS data

-   Some variables need to be recoded into simpler categories

-   For `EDUCLVL`, we may want to combine the "Secondary" and "Higher"
    groups:

::: {.fragment .fade-in}
```{r}
ke_dhs$EDUCLVL[1:5]
```
:::

## Cleaning DHS data {auto-animate="true"}

-   `case_when()` can be useful here

-   You can specify multiple *conditions* and *replacement values*:

::: {.fragment .fade-in}
```{r}
#| code-line-numbers: "|2|3|4|5"
#| eval: false
case_when(
  ke_dhs$EDUCLVL == 8 ~ NA, 
  ke_dhs$EDUCLVL >= 2 ~ "Secondary+",
  ke_dhs$EDUCLVL == 1 ~ "Primary",
  TRUE ~ "None"
)
```
:::

## Cleaning DHS data {auto-animate="true"}

::: nonincremental
-   `case_when()` can be useful here

-   You can specify multiple *conditions* and *replacement values*:
:::

```{r}
#| out.lines: 5
case_when(
  ke_dhs$EDUCLVL == 8 ~ NA, 
  ke_dhs$EDUCLVL >= 2 ~ "Secondary+",
  ke_dhs$EDUCLVL == 1 ~ "Primary",
  TRUE ~ "None"
)
```

## Cleaning DHS data

-   Fortunately, when we use dplyr, we don't have to indicate the name
    of the dataset each time:

::: {.fragment .fade-in}
```{r}
ke_dhs <- ke_dhs |>
  mutate(
    EDUCLVL = case_when(
      EDUCLVL == 8 ~ NA,
      EDUCLVL >= 2 ~ "Secondary+",
      EDUCLVL == 1 ~ "Primary",
      TRUE ~ "None"
    )
  )
```
:::

## Cleaning DHS data

-   Now we just have to do a similar process for the rest of our input
    variables!

-   We will save this as an exercise

```{r}
#| echo: false
#| results: false
ke_dhs <- ke_dhs |> 
  mutate(
    FLOOR = case_when(
      FLOOR >= 996 ~ NA,
      FLOOR >= 300 ~ "Finished",
      TRUE ~ "Unfinished"
    ),
    TOILETTYPE = case_when(
      TOILETTYPE == 0 ~ "No facility",
      TOILETTYPE >= 9996 ~ NA,
      TOILETTYPE >= 2000 ~ "Non-flush",
      TRUE ~ "Flush"
    ),
    DRINKWTR = case_when(
      DRINKWTR >= 9996 ~ NA,
      DRINKWTR >= 4000 ~ "Other",
      DRINKWTR >= 3000 ~ "Surface",
      DRINKWTR >= 2000 ~ "Well",
      TRUE ~ "Piped"
    ),
    FEVRECENT = case_when(
      FEVRECENT >= 97 ~ NA,
      FEVRECENT >= 20 ~ "Yes",
      TRUE ~ "No"
    ),
    DIARRECENT = case_when(
      DIARRECENT >= 97 ~ NA,
      DIARRECENT >= 20 ~ "Yes",
      TRUE ~ "No"
    )
  )
```

## Defining our sample {auto-animate="true"}

-   We can also use `mutate()` to create *new* columns
-   In this case, to make an ID for each child in our sample:

::: {.fragment .fade-in}
```{r}
#| eval: false
# str_squish() removes excess whitespace in the ID strings
ke_dhs <- ke_dhs |>
  mutate(KIDID = str_squish(paste(IDHSPID, BIDX)))
```

![](images/hex/stringr.png){.absolute right="10" bottom="10"
height="150"}
:::

## Defining our sample {auto-animate="true"}

::: nonincremental
-   We can also use `mutate()` to create *new* columns
-   In this case, to make an ID for each child in our sample:
    ![](images/hex/stringr.png){.absolute right="10" bottom="10"
    height="150"}
:::

```{r}
#| out.lines: 4
# str_squish() removes excess whitespace in the ID strings
ke_dhs <- ke_dhs |>
  mutate(KIDID = str_squish(paste(IDHSPID, BIDX)))

ke_dhs$KIDID
```

## Defining our sample

-   We may need to exclude certain records from our analysis

-   For instance:

    -   Missing data
    -   Unreliable data
    -   Records belong to populations that are not being considered

## Defining our sample

-   This last point is particularly important when considering climate
    exposure

-   People who do not consistently live in an area will not have been
    exposed to the climate data for that area!

-   Many DHS surveys include an indication of residency status

    -   In IPUMS DHS: `RESIDEINTYR` shows number of years at address
    -   In DHS: `V104`

## Defining our sample {auto-animate="true"}

-   Unfortunately, Kenya 2014 sample only includes a more general
    variable: `RESIDENT` (Usual resident vs. visitor)

-   We will remove all records that aren't residents of the cluster
    where they were surveyed

-   We can use `filter()` from dplyr to keep only records for "usual
    residents"

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_dhs <- ke_dhs |>
  filter(RESIDENT == 1)
```
:::

## Defining our sample {auto-animate="true"}

::: nonincremental
-   Unfortunately, Kenya 2014 sample only includes a more general
    variable: `RESIDENT` (Usual resident vs. visitor)

-   We will remove all records that aren't residents of the cluster
    where they were surveyed

-   We can use `filter()` from dplyr to keep only records for "usual
    residents"
:::

```{r}
#| out.lines: 4
ke_dhs <- ke_dhs |>
  filter(RESIDENT == 1)

ke_dhs$RESIDENT
```

## Defining our sample

-   Missing values are a common reason to remove data

-   Again, we can use `filter()` with `is.na()` to remove missing values

::: {.fragment .fade-in}
```{r}
ke_dhs <- ke_dhs |>
  filter(!is.na(BIRTHWT))
```
:::

## Data types {auto-animate="true"}

::: {.fragment .fade-in}
-   We often need to recode variables to the correct data types
-   Sometimes categorical variables are treated as numeric variables,
    but we want them to be *factors*
-   A factor is the way R stores categorical data
:::

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_dhs <- as_factor(ke_dhs) # Convert all labeled columns to factors
```
:::

## Data types {auto-animate="true"}

::: nonincremental
-   We often need to recode variables to the correct data types
-   Sometimes categorical variables are treated as numeric variables,
    but we want them to be *factors*
-   A factor is the way R stores categorical data
:::

```{r}
#| out.lines: 4
ke_dhs <- as_factor(ke_dhs) # Convert all labeled columns to factors

ke_dhs$RESIDENT
```

# R Demo

Cleaning tabular data üßº

<!-- ## Descriptives?? -->

<!-- This should be in the modeling section I think. Ask kat to do some descriptives? -->

# Questions?

# 4. DHS GPS Data

## About GPS data

::: nonincremental
::: columns
::: {.column width="60%"}
::: {.fragment .fade-in-then-semi-out fragment-index="1"}
-   Location information is collected for the household groups
    ("clusters") that were sampled in a survey
:::

::: {.fragment .fade-in-then-semi-out fragment-index="2"}
-   GPS locations represent the *centroid* of the sampled cluster
:::

::: {.fragment .fade-in-then-semi-out fragment-index="4"}
-   GPS locations are further *displaced* to protect privacy
:::
:::

::: {.column width="40%"}
::: {.fragment .fade-in-then-out fragment-index="2"}
![](images/gps.png){.absolute top="100" right="50" height="300"}
:::

::: {.fragment .fade-in fragment-index="4"}
![](images/gps_disp2.png){.absolute top="100" right="50" height="300"}
:::
:::
:::
:::

## Access to GPS data

-   Typically you must [request special
    access](https://www.dhsprogram.com/Methodology/GPS-Data.cfm) via the
    DHS program to obtain GPS coordinate data

-   ***For this workshop, we have generated fake GPS coordinate data!***

-   These data look like what you would get from the DHS Program, but
    the coordinates have been randomly generated.

## Disclaimer

*As our coordinates do not reflect real observations, all analyses are
for demonstration only!*

## Loading GPS data in R

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   We will use the [sf](https://r-spatial.github.io/sf/) package (for
    ***s**imple **f**eatures*)
:::

::: {.fragment .fade-in fragment-index="2"}
-   This allows us to represent spatial data in a familiar tabular
    format
:::

::: {.fragment .fade-in fragment-index="3"}
-   If you are familiar with GIS software, you will find that you can do
    many of the same operations using sf and R
:::
:::

::: {.fragment .fade-in fragment-index="1"}
![](images/hex/sf.png){.absolute right="10" bottom="50" height="150"}
:::

## Loading GPS data in R {auto-animate="true"}

-   Load a spatial file with `st_read()`

::: {.fragment .fade-in}
```{r}
#| eval: false
# Reminder: these are fake coordinates!
ke_gps <- st_read("data/ke_gps.shp", quiet = TRUE)
```
:::

## Loading GPS data in R {auto-animate="true"}

::: nonincremental
-   Load a spatial file with `st_read()`
:::

```{r}
#| code-line-numbers: "|5|6-9"
# Reminder: these are fake coordinates!
ke_gps <- st_read("data/ke_gps.shp", quiet = TRUE)

ke_gps
```

## Simple features

-   Note that sf is used only for **vector** data. That is, data
    composed of discrete features, like points, lines, and polygons

::: {.fragment .fade-in}
![](images/plp1.png){fig-align="center"}
:::

-   Cluster locations are represented by `POINT` geometries

# Questions?

## Mapping

-   We will use the ggspatial package to produce static maps for this
    workshop

-   ggspatial is designed to be used with the popular ggplot2 package

## Mapping {auto-animate="true"}

::: {.fragment .fade-in}
```{r}
#| eval: false
ggplot() +
  layer_spatial(ke_gps) +
  theme_minimal()
```
:::

## Mapping {auto-animate="true"}

```{r}
ggplot() +
  layer_spatial(ke_gps) +
  theme_minimal()
```

## Mapping

-   We'll obtain some borders to add some context.

-   These are from the 2014 DHS survey

::: {.fragment .fade-in}
```{r}
ke_borders <- st_read(
  "data/ke_boundaries/sdr_subnational_boundaries2.shp", 
  quiet = TRUE
)
```
:::

## Mapping {auto-animate="true"}

```{r}
#| eval: false
theme_set(theme_minimal())
```

## Mapping {auto-animate="true"}

```{r}
#| eval: false
#| code-line-numbers: "|4"
theme_set(theme_minimal())

ggplot() +
  layer_spatial(ke_borders) +
  layer_spatial(ke_gps)
```

## Mapping {auto-animate="true"}

```{r}
#| code-line-numbers: "4"
theme_set(theme_minimal())

ggplot() +
  layer_spatial(ke_borders) +
  layer_spatial(ke_gps)
```

## Modifying map style {auto-animate="true"}

-   We can modify the aesthetics of the map as well as the content

::: {.fragment .fade-in}
```{r}
#| eval: false
#| code-line-numbers: "|4-6|10-11|13"
ggplot() +
  layer_spatial(
    ke_borders,
    fill = "#F6E6CB",  # Polygon fill color
    color = "#7f7f7f", # Polygon outline color
    linewidth = 0.5    # Polygon line width
  ) +
  layer_spatial(
    ke_gps,
    alpha = 0.2,       # Cluster point transparency
    color = "#444444"  # Cluster point color
  ) +
  labs(title = "DHS Cluster Locations", subtitle = "Note: falsified coordinates")
```
:::

## Modifying map style {auto-animate="true"}

::: nonincremental
-   We can modify the aesthetics of the map as well as the content
:::

```{r}
#| echo: false
ggplot() +
  layer_spatial(
    ke_borders,
    fill = "#F6E6CB",  # Polygon fill color
    color = "#7f7f7f", # Polygon outline color
    linewidth = 0.5    # Polygon line width
  ) +
  layer_spatial(
    ke_gps,
    alpha = 0.2,       # Cluster point transparency
    color = "#444444"  # Cluster point color
  ) +
  labs(title = "DHS Cluster Locations", subtitle = "Note: falsified coordinates")
```

# R Demo

Vector data and ggspatial üó∫Ô∏è

::: notes
Demo contents? - Probably mostly ggplot2 stuff... - aesthetics of
layers - Coloring by features - theming?

Exercises: - For the counties identified in the first exercise, make a
map where you color them something different than the other counties?
:::

# Questions?

# 5. Climate Data

## Data sources

::: nonincremental
-   We are focusing on **extreme heat**

::: {.fragment .fade-in fragment-index="1"}
-   We will use [CHIRTS](https://www.chc.ucsb.edu/data/chirtsdaily) data
:::

::: {.fragment .fade-in fragment-index="2"}
-   CHIRTS is provided by the [Climate Hazards
    Center](https://chc.ucsb.edu/) (CHC)
:::

::: {.fragment .fade-in fragment-index="2"}
![](images/CHC.png){.absolute right="50" bottom="20" height="200"}
:::
:::

## What is CHIRTS?

-   CHIRTS provides daily estimates for several temperature metrics at a
    0.05¬∞ (\~5 kilometer) resolution

-   Data for 1983-2016

-   Multiple temperature metrics to choose from (max, min, relative
    humidity, etc.)

-   For this demonstration, we will use **daily maximum air
    temperature** (Tmax)

## What is CHIRTS?

-   Like many climate data sources, CHIRTS is a **raster** data source

-   Compared to vector data, raster data represent geographic features
    in a grid

    -   Each cell in the grid is associated with a particular value

::: {.fragment .fade-in}
![](images/plp2.png){fig-align="center"}
:::

## Obtaining CHIRTS data

-   **Option 1:** [Direct
    download](http://data.chc.ucsb.edu/products/CHIRTSdaily/v1.0/africa_netcdf_p05/)

-   Can download yearly data for Africa at different resolutions

::: {.fragment .fade-in}
![](images/chirts_download.png){fig-align="center" height="450"}
:::

## Obtaining CHIRTS data

-   **Option 2:** The
    [`chirps`](https://docs.ropensci.org/chirps/index.html) package in R

-   Can provide parameters indicating the desired data right in your R
    console!

::: {.fragment .fade-in}
```{r}
#| eval: false
library(chirps)

ke_chirts <- get_chirts(
  vect(ke_borders),                      # Spatial range
  dates = c("2013-01-01", "2013-12-31"), # Date range
  var = "Tmax"                           # Desired temperature variable
)
```
:::

## Obtaining CHIRTS data

-   We have already prepared and saved some CHIRTS data, since it can
    take time/space to download and store raster data

-   File `ke_chirts_2013.nc` in the `data` directory of the workshop
    materials

## Raster data

-   Think "image" files.
-   Like a camera: a rectangular set of pixels at a certain resolution

::: {.fragment .fade-in}
```{r}
#| echo: false
ke_chirts <- rast("data/ke_chirts_2013.nc") # Our pre-saved CHIRTS NetCDF

ke_chirts <- subst(ke_chirts, -9999, NA)

plot(ke_chirts[[1]])
```
:::

## Raster data in R

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   sf does not handle raster data
:::

::: {.fragment .fade-in fragment-index="2"}
-   Instead, we will use
    [terra](https://rspatial.github.io/terra/index.html)
:::

::: {.fragment .fade-in fragment-index="3"}
-   Unlike vector data, raster data is not easily handled in a tabular
    format
:::

::: {.fragment .fade-in fragment-index="4"}
-   terra uses its own set of R data structures to represent raster data
:::
:::

::: {.fragment .fade-in fragment-index="2"}
![](images/hex/terra.png){.absolute right="10" bottom="50" height="150"}
:::

## Raster data in R {auto-animate="true"}

-   How does terra represent rasters?

-   Load a raster with `rast()`

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_chirts <- rast("data/ke_chirts_2013.nc") # Our pre-saved CHIRTS NetCDF
```
:::

## Raster data in R {auto-animate="true"}

::: nonincremental
-   How does terra represent rasters?

-   Load a raster with `rast()`
:::

```{r}
#| code-line-numbers: "|4|6-7|8|10-11"
ke_chirts <- rast("data/ke_chirts_2013.nc") # Our pre-saved CHIRTS NetCDF

ke_chirts
```

## Raster data in R

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   Rasters can be composed of multiple layers (a *raster stack*)
:::

::: {.fragment .fade-in fragment-index="2"}
-   Often, each layer represents a single day of data
:::
:::

::: {.fragment .fade-in fragment-index="1"}
![](images/raster_stack.png){fig-align="center" height="400"}
:::

## Raster data in R {auto-animate="true"}

-   Access specific layers with `[[`

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_chirts[[1]]
```
:::

## Raster data in R {auto-animate="true"}

::: nonincremental
-   Access specific layers with `[[`
:::

```{r}
#| code-line-numbers: "|3,8-9"
ke_chirts[[1]]
```

## Raster data in R {auto-animate="true"}

-   You can access a variety of details about the raster using terra

::: {.fragment .fade-in}
```{r}
#| eval: false
#| cache: true
# Get raster cell values (in this case, max temperature for that cell)
values(ke_chirts)
```
:::

## Raster data in R {auto-animate="true"}

::: nonincremental
-   You can access a variety of details about the raster using terra
:::

```{r}
#| out.lines: 5
#| cache: true
# Get raster cell values (in this case, max temperature for that cell)
values(ke_chirts)
```

## Raster data in R {auto-animate="true"}

::: nonincremental
-   You can access a variety of details about the raster using terra
:::

```{r}
#| eval: false
# Number of layers
nlyr(ke_chirts)
```

## Raster data in R {auto-animate="true"}

::: nonincremental
-   You can access a variety of details about the raster using terra
:::

```{r}
# Number of layers
nlyr(ke_chirts)
```

## Raster data in R {auto-animate="true"}

::: nonincremental
-   You can access a variety of details about the raster using terra
:::

```{r}
#| eval: false
# Date that corresponds to each layer
time(ke_chirts)
```

## Raster data in R {auto-animate="true"}

::: nonincremental
-   You can access a variety of details about the raster using terra
:::

```{r}
#| out.lines: 5
# Date that corresponds to each layer
time(ke_chirts)
```

-   `time()` will be useful later!

## Mapping with raster data {auto-animate="true"}

-   ggspatial works with raster data as well!

::: {.fragment .fade-in}
```{r}
#| eval: false
# Reclassify missing values
ke_chirts <- subst(ke_chirts, -9999, NA)
```
:::

## Mapping with raster data {auto-animate="true"}

::: nonincremental
-   ggspatial works with raster data as well!
:::

```{r}
#| eval: false
#| code-line-numbers: "|5|6"
# Reclassify missing values
ke_chirts <- subst(ke_chirts, -9999, NA)

ggplot() +
  layer_spatial(ke_chirts[[1]], alpha = 0.9) +
  layer_spatial(ke_borders, fill = NA, color = "#4c4c4c", linewidth = 0.5)
```

## Mapping with raster data {auto-animate="true"}

::: nonincremental
-   ggspatial works with raster data as well!
:::

```{r}
#| echo: false
#| fig-align: center
# Reclassify missing values
ke_chirts <- subst(ke_chirts, -9999, NA)

ggplot() +
  layer_spatial(ke_chirts[[1]], alpha = 0.9) +
  layer_spatial(ke_borders, fill = NA, color = "#4c4c4c", linewidth = 0.5)
```

# R Demo

Working with raster data üåê

::: notes
Demo contents? Should just be basics of interacting with rasters we will
get to complex stuff later - Raster layers - Accessing layers - Baisc
aggregation (e.g. `mean()`, `max()`, etc.) - Cropping - Calculating new
values (`app()`. `tapp()` will be later) - Plotting
:::

# Questions?

# 6. Working with Space and Time

## Thinking spatially

-   Spatial data sounds flashy, but we typically have to simplify it
    before it can be useful

-   We have detailed daily maximum temperature for an entire country

-   But our DHS survey records are recorded at particular locations

::: {.fragment .fade-in}
*How do we convert our climate data into a form that can be used with
DHS surveys?*
:::

## Thinking spatially

::: nonincremental
Adding spatial data to our analyses requires two streams of thought:

::: {.fragment .fade-in}
-   **Conceptual framework**
    -   How does temperature impact health? Through which pathways?
:::

::: {.fragment .fade-in}
-   **Technical processing**
    -   We need to convert our data sources into a format that reflects
        the identified pathways
:::
:::

# Brainstorm

::: notes
Could have a quick brainstorm about possible temp. metrics and what the
challenges would be to measure each of them, etc.
:::

# 6.1 Transforming Vector Data

## Buffering

-   Recall our GPS clusters are represented as *points*

-   Climate effects rarely happen only at a single point!

-   The climate of the *general area* around a cluster might provide a
    better representation of how it impacts the population in an area

## Buffering

-   We can capture this by creating a *spatial buffer*, or a area around
    each cluster point

-   However, we first need to deal with another technical detail

## Projecting

-   Our data are currently in latitude/longitude format:

::: {.fragment .fade-in}
```{r}
#| code-line-numbers: "|5-6,9-11"
#| out.lines: 10
ke_gps[, "geometry"]
```
:::

## Projecting

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   Number of meters in a degree *varies* across the globe
    -   Especially for longitude
:::

::: {.fragment .fade-in fragment-index="2"}
-   This means our buffer zone size would vary by the location of each
    cluster!
:::

::: {.fragment .fade-in fragment-index="1"}
![](images/graticule.png){.absolute bottom="30" right="50" height="350"}
:::
:::

## Projecting

-   We need to first transform our coordinate system so that our units
    are in meters

-   We need to *project* our spherical coordinates onto a flat surface

::: {.fragment .fade-in}
![](images/map_proj.png){fig-align="center" height="400"}
:::

::: notes
Source:
https://gistbok.ucgis.org/bok-topics/2017-quarter-03/map-projections
:::

## Selecting a Projection

::: nonincremental
::: {.fragment .fade-in fragment-index="1"}
-   Projections inherently distort the spherical earth
:::

::: {.fragment .fade-in fragment-index="2"}
-   However, different projections preserve different attributes of
    space

    -   Distance
    -   Area
    -   Shapes
    -   A compromise
:::
:::

::: {.fragment .fade-in fragment-index="1"}
![](images/proj_distortion.jpg){.absolute right="100" bottom="20"
height="400"}
:::

## Selecting a Projection

-   There are also projection options designed to minimize distortion in
    specific areas
-   One option: Universal Transverse Mercator (UTM)
-   UTM has different zones for different areas on the globe

::: {.fragment .fade-in}
![](images/utm_zones.png){.absolute right="100" bottom="20"
height="300"}
:::

## Selecting a Projection

-   Zone 37 provides the best projection (least distortion) for Kenya
-   Kenya is split along the border between 37N and 37S
-   For simplicity we will use 37N
-   UTM 37N corresponds to **EPSG 32637**

## Projection in R {auto-animate="true"}

-   Use sf's `st_transform()` to project

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_gps <- st_transform(ke_gps, crs = 32637)
```
:::

## Projection in R {auto-animate="true"}

::: nonincremental
-   Use sf's `st_transform()` to project
:::

```{r}
#| code-line-numbers: "|7-8,11-13"
#| out.lines: 10
ke_gps <- st_transform(ke_gps, crs = 32637)

ke_gps[, "geometry"]
```

## Projection in R

-   We can still plot just as before

::: {.fragment .fade-in}
```{r}
#| fig-align: center
ggplot() +
  layer_spatial(ke_gps)
```
:::

## Projection in R

::: nonincremental
-   The map looks pretty much the same!
-   If we were to use a less appropriate projection, the difference
    would be more obvious (see exercises)
:::

## Creating Buffers {auto-animate="true"}

-   Now that our data are in meters, we can buffer
-   We will use a 10km buffer in this case
-   In the real world, this decision should be based on your research
    question or the geography of interest!

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_buffer <- st_buffer(ke_gps, dist = 10000)
```
:::

## Creating Buffers {auto-animate="true"}

::: nonincremental
-   Now that our data are in meters, we can buffer
-   We will use a 10km buffer in this case
-   In the real world, this decision should be based on your research
    question or the geography of interest!
:::

```{r}
#| code-line-numbers: "|11-13"
#| out.lines: 10
ke_buffer <- st_buffer(ke_gps, dist = 10000)

ke_buffer[, "geometry"]
```

## Creating Buffers

::: {.fragment .fade-in}
```{r}
#| fig-align: center
ggplot() +
  layer_spatial(ke_buffer, alpha = 0)
```
:::

## Transform Back

::: nonincremental
::: {.fragment .fade-in}
-   We can return back to our latitude/longitude coordinates now that we
    have constructed our buffer zones

```{r}
ke_buffer <- st_transform(ke_buffer, crs = 4326) # WGS 84 CRS
```
:::
:::

# R Demo

Projections üåç

# Questions?

# 6.2 Transforming Raster Data

## Temporal aggregation

-   Our CHIRTS data form a *daily time series*
-   But the children in our DHS survey were born at specific points in
    time
-   We need to include temperature data from the *relevant time period*
    for each child

## Temporal aggregation

::: nonincremental
::: {.fragment .fade-in}
-   How to simplify our CHIRTS data into temperature exposure summaries
    for each child?
:::

::: {.fragment .fade-in}
**Considerations:**
:::

::: {.fragment .fade-in}
-   What temporal level do we want to summarize to?
    -   *Days? Months? A single summary value?*
:::

::: {.fragment .fade-in}
-   How to summarize daily data?
    -   *Average temperature?*
    -   *Number of days above a given temperature?*
    -   ...
:::
:::

## Average temperature {auto-animate="true"}

-   Use terra!
-   If aggregating the entire time series, use a single summary
    function:

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_chirts_mean <- mean(ke_chirts)
```
:::

## Average temperature {auto-animate="true"}

::: nonincremental
-   Use terra!
-   If aggregating the entire time series, use a single summary
    function:
:::

```{r}
#| code-line-numbers: "|5,10-12"
ke_chirts_mean <- mean(ke_chirts)

ke_chirts_mean
```

## Average temperature {auto-animate="true"}

-   Note that this aggregates **cells** across **layers** (time), so we
    will still have data for our full *spatial* region

::: {.fragment .fade-in}
```{r}
#| echo: false
#| fig-align: center
theme_set(theme_dhs_base())

ggplot() +
  layer_spatial(
    mask(ke_chirts_mean, ke_borders),
    alpha = 0.9
  ) +
  layer_spatial(ke_borders, fill = NA, color = "#4c4c4c", linewidth = 0.5) +
  labs(title = "Mean Temperature (¬∞C)", subtitle = "Kenya, 2013", fill = "") +
  scale_fill_chirts_c()
```
:::

## Monthly aggregation

-   This approach produces an average for the *entire* time series of
    CHIRTS data

-   The children in our DHS survey were born at *specific* points in
    time

-   This single average does not reflect the conditions that each child
    has experienced!

## Monthly aggregation

-   DHS collects birth date data at the monthly level

-   We can summarize our temperature data to the *monthly* level

    -   Then we can isolate the temperature conditions **around each
        child's birth date**

## Monthly aggregation {auto-animate="true"}

-   Recall that our raster has a `time` component

-   We can use this to aggregate our raster to the monthly level

-   Use `tapp()` to summarize for specific layer (time) groups:

::: {.fragment .fade-in}
```{r}
#| eval: false
#| code-line-numbers: "|3|4|5"
# Average daily CHIRTS within months
ke_chirts_mean_mnths <- tapp(
  ke_chirts,
  fun = mean,
  index = "yearmonths"
)
```
:::

## Monthly aggregation {auto-animate="true"}

::: nonincremental
-   Recall that our raster has a `time` component

-   We can use this to aggregate our raster to the monthly level

-   Use `tapp()` to summarize for specific layer (time) groups:
:::

```{r}
#| code-line-numbers: "|10,15,18"
# Average daily CHIRTS within months
ke_chirts_mean_mnths <- tapp(
  ke_chirts,
  fun = mean,
  index = "yearmonths"
)

ke_chirts_mean_mnths
```

<!-- ::: {.fragment .fade-in} -->

<!-- ```{r} -->

<!-- #| code-line-numbers: "|2|3|4|" -->

<!-- #| eval: false -->

<!-- ke_count_above_35_months <- tapp( -->

<!--   ke_bin,              # Input raster -->

<!--   fun = mean,          # Calculate mean (i.e. proportion of days above 35) -->

<!--   index = "yearmonths" # Group by year + month -->

<!-- ) -->

<!-- ``` -->

<!-- ::: -->

<!-- --- -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- ke_count_above_35_months <- tapp( -->

<!--   ke_bin, # Input raster -->

<!--   fun = mean, # Calculate mean (i.e. proportion of days above 35) -->

<!--   index = "yearmonths" # Group by year + month -->

<!-- ) -->

<!-- months <- c("January", "February", "March", "April", -->

<!--             "May", "June", "July", "August", -->

<!--             "September", "October", "November", "December") -->

<!-- panels <- purrr::map2( -->

<!--   split_raster(ke_count_above_35_months), -->

<!--   months, -->

<!--   function(r, m) { -->

<!--     chirts_panel_continuous( -->

<!--       r, -->

<!--       borders = ke_borders, -->

<!--       panel_title = m, -->

<!--       show_scale = FALSE, -->

<!--       fill_lab = "" -->

<!--     ) -->

<!--   } -->

<!-- ) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-align: center -->

<!-- patchwork::wrap_plots(panels) + -->

<!--   patchwork::plot_layout(guides = "collect", ncol = 6) + -->

<!--   patchwork::plot_annotation( -->

<!--     title = "Proportion of Days above 35¬∞C", -->

<!--     subtitle = "Kenya, 2013", -->

<!--     caption = "Source: Climate Hazards Center" -->

<!--   ) -->

<!-- ``` -->

------------------------------------------------------------------------

```{r}
#| echo: false
#| cache: true
months <- c("January", "February", "March", "April",
            "May", "June", "July", "August",
            "September", "October", "November", "December")

panels <- purrr::map2(
  split_raster(ke_chirts_mean_mnths),
  months,
  function(r, m) {
    chirts_panel_continuous(
      r,
      borders = ke_borders,
      panel_title = m,
      show_scale = FALSE,
      fill_lab = "",
      limits = c(7, 44)
    )
  }
)
```

```{r}
#| echo: false
#| cache: true
#| fig-align: center
patchwork::wrap_plots(panels) +
  patchwork::plot_layout(guides = "collect", ncol = 6) +
  patchwork::plot_annotation(
    title = "Average Monthly Temperature (¬∞C)",
    subtitle = "Kenya, 2013",
    caption = "Source: Climate Hazards Center"
  )
```

## Temperature deviation

-   It can sometimes be useful to consider temperature in context of the
    "normal" for a region

-   What is considered "hot" in one area might not be in another area
    with a different baseline climate

## Temperature deviation

-   We just calculated a monthly average temperature

-   We can subtract it from the "typical" temperature for that month

## Temperature deviation {auto-animate="true"}

-   To speed things up, we have already created rasters containing the
    10-year average temperature for each month

::: {.fragment .fade-in}
```{r}
#| eval: false
# List all 12 monthly average files
chirts_mnth_means <- list.files("data/chirts", full.names = TRUE)
```
:::

## Temperature deviation {auto-animate="true"}

::: nonincremental
-   To speed things up, we have already created rasters containing the
    10-year average temperature for each month
:::

```{r}
# List all 12 monthly average files
chirts_mnth_means <- list.files("data/chirts", full.names = TRUE)

# Load monthly averages into a single raster stack
chirts_norm <- rast(chirts_mnth_means)
```

## Temperature deviation

::: nonincremental
::: {.fragment .fade-in}
-   We can perform arithmetic between rasters as well
:::

::: {.fragment .fade-in}
-   We have average monthly temperature

    -   For 2013
    -   For 2000-2009 (our "normal")
:::

::: {.fragment .fade-in}
-   We can subtract the rasters from each other layer by layer
:::
:::

## Temperature deviation {auto-animate="true"}

```{r}
#| eval: false
# Subtract 2013 monthly average from 10-year monthly average
chirts_dev <- ke_chirts_mean_mnths - chirts_norm
```

## Temperature deviation {auto-animate="true"}

```{r}
# Subtract 2013 monthly average from 10-year monthly average
chirts_dev <- ke_chirts_mean_mnths - chirts_norm

chirts_dev
```

## Temperature deviation

-   We still have a raster, but the values now reference *differences*
    between 2013 temperature and a "normal" temperature

::: {.fragment .fade-in}
```{r}
#| echo: false
#| cache: true
ggplot() +
  layer_spatial(chirts_dev[[5]]) +
  scale_fill_gradient2(
    low = "#438b81",
    mid = "#f0ebd7",
    high = "#c25d33",
    limits = c(-2, 2),
    na.value = "transparent"
  ) +
  labs(
    title = "Difference from 10-year average temperature (¬∞C)",
    subtitle = "May, 2013",
    fill = ""
  )
```
:::

------------------------------------------------------------------------

```{r}
#| echo: false
#| cache: true
panels <- purrr::map2(
  split_raster(chirts_dev),
  months,
  function(r, m) {
    chirts_panel_diverging(
      r,
      borders = ke_borders,
      panel_title = m,
      show_scale = FALSE,
      fill_lab = "",
      low = "#438b81", mid = "#f0ebd7", high = "#c25d33",
      limits = c(-2, 2),
      na.value = "transparent"
    )
  }
)
```

```{r}
#| echo: false
#| fig-align: center
#| cache: true
patchwork::wrap_plots(panels) +
  patchwork::plot_layout(guides = "collect", ncol = 6) +
  patchwork::plot_annotation(
    title = "Deviation from 10-Year Monthly Average (¬∞C)",
    subtitle = "Kenya, 2013",
    caption = "Source: Climate Hazards Center"
  )
```

## Other aggregation approaches?

-   We will explore some more in the demo for this section!

-   For the rest of our demo, we'll use *average monthly temperature* to
    keep things simple

# Questions?

## Spatial aggregation

-   Now we have our raster aggregated temporally
-   How do we identify the temperature values at our cluster locations?

::: {.fragment .fade-in}
```{r}
#| echo: false
#| fig-align: center
ke_mean_to_plot <- mask(ke_chirts_mean_mnths[[1]], ke_borders)

ggplot() +
  layer_spatial(ke_mean_to_plot, alpha = 0.9) +
  layer_spatial(ke_borders, fill = NA, color = "#444444") +
  layer_spatial(ke_buffer, fill = NA, color = alpha("black", 0.5)) +
  labs(title = "Mean Temperature (¬∞C)", subtitle = "January, 2013", fill = "") +
  scale_fill_chirts_c()
```
:::

## Zooming in

```{r}
# Select a single cluster to demonstrate
clust1 <- ke_buffer |>
  filter(DHSID == "KE201400001487")
```

::: {.fragment .fade-in}
```{r}
#| echo: false
#| fig-align: center
ggplot() +
  layer_spatial(crop(ke_chirts_mean_mnths[[1]], clust1, snap = "out")) +
  layer_spatial(clust1, fill = NA, color = "black") +
  labs(title = "Mean Temperature (¬∞C)", subtitle = "Single Cluster", fill = "") +
  scale_fill_chirts_c()
```
:::

## Spatial aggregation

-   We can use terra's `extract()`

-   Let's experiment with the sample cluster from before

-   By default, it extracts the cell values within the cluster region:

::: {.fragment .fade-in}
```{r}
extract(ke_chirts_mean_mnths[[1]], clust1)
```
:::

## Spatial aggregation

-   Note that this includes only the 10 cells whose center lies within
    our cluster polygon.
-   Use `weights = TRUE` to get the weights for each cell based on
    intersection area:

::: {.fragment .fade-in}
```{r}
#| out.lines: 10
extract(
  ke_chirts_mean_mnths[[1]],
  clust1,
  weights = TRUE
)
```
:::

## Spatial aggregation

-   We can *summarize* the values within the cluster region as well.
-   If `weights = TRUE`, the value will automatically be area-weighted

::: {.fragment .fade-in}
```{r}
extract(
  ke_chirts_mean_mnths[[1]],
  clust1,
  weights = TRUE,
  fun = mean
)
```
:::

## Scaling up

-   We can easily apply the same operation to all of our cluster regions
    at once

::: {.fragment .fade-in}
```{r}
#| cache: true
#| code-line-numbers: "|2-3"
ke_chirts_clust <- extract(
  ke_chirts_mean_mnths, # Extract values for each month of temperature data
  ke_buffer,            # Use all cluster polygons
  weights = TRUE,
  fun = mean
)
```
:::

## Scaling up {auto-animate="true"}

::: nonincremental
-   It will be useful to use the `DHSID` cluster ID instead of the index
    ID that `extract()` produces.
:::

::: {.fragment .fade-in}
```{r}
#| eval: false
# Update to use cluster IDs
ke_chirts_clust$ID <- ke_buffer$DHSID
```
:::

## Scaling up {auto-animate="true"}

::: nonincremental
-   It will be useful to use the `DHSID` cluster ID instead of the index
    ID that `extract()` produces.
:::

```{r}
#| out.lines: 5
# Update to use cluster IDs
ke_chirts_clust$ID <- ke_buffer$DHSID

ke_chirts_clust
```

-   Now each cluster ID is associated with 12 months of CHIRTS data!

# R Demo

Raster aggregation üåç

# Questions?

::: notes
A good chance to remind that we can calculate all sorts of things in R
but they should always be motivated by the geography + research
questions and ground-level reality so that the results are meaningful
approximations of real-world impact pathways
:::

# 7. Linking climate exposure to survey data

# 7.1 Data source formats

## Wide vs. long format

-   `extract()` produces data in **wide** format

-   Each *column* corresponds to a month of temperature data for a given
    cluster

-   This is a common format in GIS software

::: {.fragment .fade-in}
```{r}
#| out.lines: 5
ke_chirts_clust
```
:::

## Wide vs. long format

-   However, tabular data is often handled in **long** format

-   Each *row* corresponds to a month of temperature data for a given
    cluster

## Wide vs. long format {auto-animate="true"}

-   We can convert our cluster data to long format with `pivot_longer()`

::: {.fragment .fade-in}
```{r}
#| out.lines: 5
ke_chirts_long <- ke_chirts_clust |>
  pivot_longer(
    cols = -ID, # Do not pivot the ID col
    names_to = "CHIRTS_DATE", # Rename output columns
    values_to = "MEAN_TEMP"
  )
```
:::

## Wide vs. long format {auto-animate="true"}

::: nonincremental
-   We had 1594 clusters and 12 months of data per cluster, so we should
    end up with `1594 * 12 = 19128` rows

```{r}
#| out.lines: 10
ke_chirts_long <- ke_chirts_clust |>
  pivot_longer(
    cols = -ID, # Do not pivot the ID col
    names_to = "CHIRTS_DATE", # Rename output columns
    values_to = "MEAN_TEMP"
  )

ke_chirts_long
```
:::

## Dates in DHS

-   DHS uses **century-month codes** (CMCs) to represent dates
-   CMC codes represent dates as the number of months that have elapsed
    since year 1900

::: {.fragment .fade-in}
```{r}
#| out.lines: 5
ke_dhs$KIDDOBCMC
```
:::

## Dates in CHIRTS

-   However, our extracted monthly CHIRTS data is stored as a string:

::: {.fragment .fade-in}
```{r}
#| out.lines: 5
ke_chirts_long$CHIRTS_DATE
```
:::

## Converting Dates

-   We'll need to convert these to a single format before we can use the
    two data sources together
-   Fortunately, the IPUMS DHS variable info describes how to convert
    between CMC codes

::: {.fragment .fade-in}
```{r}
#| eval: false
ipums_var_desc(ke_ddi, "KIDDOBCMC")
#> ...
#> Starting with CMC figures, one can calculate the month and year using the
#> following formulas:
#>
#> Year = int( ( CMC minus 1 )/12 ) + 1900 [int(x) is the integer part of x]
#>
#> Month = CMC minus ( ( Year minus 1900 ) * 12 )
#> ...
```
:::

## Converting Dates {auto-animate="true"}

-   We can use the indicated formula to convert our CHIRTS dates into
    CMC format
-   First, we need to convert the string format to year and month
-   We will remove the `"ym_"` prefix first:

::: {.fragment .fade-in}
```{r}
#| eval: false
# Replace "ym_" with ""
ke_chirts_long <- ke_chirts_long |>
  mutate(CHIRTS_DATE = str_replace(CHIRTS_DATE, "ym_", ""))
```
:::

## Converting Dates {auto-animate="true"}

::: nonincremental
-   We can use the indicated formula to convert our CHIRTS dates into
    CMC format
-   First, we need to convert the string format to year and month
-   We will remove the `"ym_"` prefix first:
:::

```{r}
#| out.lines: 7
# Replace "ym_" with ""
ke_chirts_long <- ke_chirts_long |>
  mutate(CHIRTS_DATE = str_replace(CHIRTS_DATE, "ym_", ""))

ke_chirts_long
```

## Converting Dates {auto-animate="true"}

-   Convert the combined year-month format into a standard date
-   We can use `ym()` from the handy lubridate package

::: {.fragment .fade-in}
```{r}
#| eval: false
library(lubridate)

ke_chirts_long <- ke_chirts_long |>
  mutate(CHIRTS_DATE = ym(CHIRTS_DATE))
```
:::

## Converting Dates {auto-animate="true"}

::: nonincremental
-   Convert the combined year-month format into a standard date
-   We can use `ym()` from the handy lubridate package
:::

```{r}
#| out.lines: 8
library(lubridate)

ke_chirts_long <- ke_chirts_long |>
  mutate(CHIRTS_DATE = ym(CHIRTS_DATE))

ke_chirts_long
```

## Converting Dates

-   Finally, we can convert these dates into CMC codes using the formula
    from before

    -   `(year - 1900) * 12 + month`

::: {.fragment .fade-in}
```{r}
#| out.lines: 5
ke_chirts_long <- ke_chirts_long |>
  mutate(CHIRTS_DATE = (year(CHIRTS_DATE) - 1900) * 12 + month(CHIRTS_DATE))

ke_chirts_long
```
:::

## Workflow note

::: nonincremental
-   We can combine all the above steps into a single R expression!
:::

::: {.fragment .fade-in}
```{r}
#| eval: false
ke_chirts_long <- ke_chirts_clust |>
  pivot_longer(
    cols = -ID,
    names_to = "CHIRTS_DATE",
    values_to = "MEAN_TEMP"
  ) |>
  mutate(
    CHIRTS_DATE = str_replace(CHIRTS_DATE, "ym_", ""),
    CHIRTS_DATE = ym(CHIRTS_DATE),
    CHIRTS_DATE = (year(CHIRTS_DATE) - 1900) * 12 + month(CHIRTS_DATE)
  )
```
:::

# 7.2 Joining data sources

## What we have at this point

-   Our DHS and CHIRTS data now have been reformatted so we can link
    records

-   Let's peek at what we have:

::: {.fragment .fade-in}
```{r}
# Cluster-level monthly CHIRTS data
ke_chirts_long
```
:::

## What we have at this point

::: nonincremental
-   Our DHS and CHIRTS data now have been reformatted so we can link
    records

-   Let's peek at what we have:
:::

```{r}
# Child-level tabular DHS survey data
ke_dhs[, c("DHSID", "KIDID", "KIDDOBCMC", "BIRTHWT")]
```

## Joining climate and DHS data

-   To join, we need to indicate:

    -   The type of join (left, right, inner, full)
    -   How records should be matched to each other

## Joining climate and DHS data {auto-animate="true"}

-   In this case, we only want to retain records that have values in
    *both* the tabular data *and* the climate data

-   This corresponds to an "inner" join

::: {.fragment .fade-in}
```{r}
#| eval: false
inner_join(
  ke_dhs,
  ke_chirts_long
)
```
:::

## Joining climate and DHS data {auto-animate="true"}

::: nonincremental
-   In this case, we only want to retain records that have values in
    *both* the tabular data *and* the climate data

-   This corresponds to an "inner" join
:::

```{r}
#| error: true
#| code-line-numbers: "|5,6"
inner_join(
  ke_dhs,
  ke_chirts_long
)
```

## Joining climate and DHS data

-   We want to match records such that:

    -   Each kid is linked to the climate data for their ***cluster***
    -   Each kid is linked to the climate data for the ***month of their
        birth***

## Joining climate and DHS data {auto-animate="true"}

::: nonincremental
-   To match records based on ***cluster***:

    -   The `DHSID` in the DHS data must match the `ID` in the CHIRTS
        monthly data

::: {.fragment .fade-in}
-   To match records based on ***month***:

    -   The child date of birth (`KIDDOBCMC`) must match the month of
        CHIRTS data (`CHIRTS_DATE`)
:::
:::

::: {.fragment .fade-in}
```{r}
#| code-line-numbers: "|4"
ke_dhs_chirts <- inner_join(
  ke_dhs,
  ke_chirts_long,
  by = c("DHSID" = "ID", "KIDDOBCMC" = "CHIRTS_DATE")
)
```
:::

## Joining climate and DHS data

```{r}
# Each child is now associated with monthly average temperature for their
# birth month
ke_dhs_chirts |>
  select(DHSID, KIDID, KIDDOBCMC, MEAN_TEMP, BIRTHWT)
```

## Exploring our joined dataset

-   Reminder: these are fake data, so we don't expect a relationship!

::: {.fragment .fade-in}
```{r}
#| fig-align: center
ggplot(ke_dhs_chirts) +
  geom_point(aes(x = MEAN_TEMP, y = BIRTHWT), alpha = 0.5, size = 2)
```
:::

# R demo

Joining data sources üß≤

::: notes
Joining based on 3 months prior to birth or something? Harder because we
only have 1 year of data...

Try different join types (left, right, full)...how do they differ? When
are they useful?
:::

# Questions?

# 8. Multilevel modeling

This and beyond is going to come from Kat/Becca

# R demo

Building a model üìà

# Questions?

# Thank you + acknowledgemnts
